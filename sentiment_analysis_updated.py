# -*- coding: utf-8 -*-
"""Sentiment_Analysis_Updated.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kNhqMXk-Lnvejn1nJH8KdUdiYIKbBtAJ
"""

! pip install sentence-transformers
! pip install gensim

import pandas as pd

# Load your dataset
df = pd.read_csv('twitter_training.csv', names=['id', 'company', 'sentiments', 'review'])

# Drop rows where sentiments are 'Irrelevant'
df = df[df['sentiments'] != 'Irrelevant']

# Optionally, reset the index of the DataFrame
df.reset_index(drop=True, inplace=True)

# Verify the change
print(df['sentiments'].value_counts())

import nltk

# Download the stopwords dataset
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

import numpy as np
import pandas as pd
import re
from nltk.tokenize import word_tokenize
from nltk.data import find
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from gensim.models import KeyedVectors
from sentence_transformers import SentenceTransformer
from sklearn.ensemble import RandomForestClassifier

# Preprocess text
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()
def preprocess_text(text):
    text = re.sub(r'http\S+|www\S+|https\S+|\@\w+|\#|\d+', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    text = text.lower()
    tokens = word_tokenize(text)
    filtered_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    return ' '.join(filtered_tokens)

df['review'] = df['review'].astype(str)
df['processed_review'] = df['review'].apply(preprocess_text)
y = df['sentiments']

print(df.columns)

vectorizer = CountVectorizer(ngram_range=(1, 2))  # Using both unigrams and bigrams
X_bow = vectorizer.fit_transform(df['processed_review'])

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X_bow, y, test_size=0.2, random_state=42)

# Initialize and fit the RandomForestClassifier model
rf_model = RandomForestClassifier()
rf_model.fit(X_train, y_train)

# Make predictions and evaluate
y_pred = rf_model.predict(X_test)
print("CountVectorizer Accuracy:", accuracy_score(y_test, y_pred))
print("CountVectorizer Classification Report:\n", classification_report(y_test, y_pred))

vectorizer = TfidfVectorizer(ngram_range=(1, 2))
X_tfidf = vectorizer.fit_transform(df['processed_review'])

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)

# Initialize and fit the RandomForestClassifier model
rf_model = RandomForestClassifier()
rf_model.fit(X_train, y_train)

# Make predictions and evaluate
y_pred = rf_model.predict(X_test)
print("TF-IDF Accuracy:", accuracy_score(y_test, y_pred))
print("TF-IDF Classification Report:\n", classification_report(y_test, y_pred))

import gensim.downloader as api

# Download the Google News pretrained Word2Vec model
w2v_model = api.load('word2vec-google-news-300')

w2v_model = KeyedVectors.load_word2vec_format('/content/GoogleNews-vectors-negative300.bin.gz', binary=True)


def vectorize_review(tokens, model):
    vectors = [model[token] for token in tokens if token in model]
    if len(vectors) == 0:
        return np.zeros(model.vector_size)
    return np.mean(vectors, axis=0)

# Tokenize the text data
tokenized_reviews = df['processed_review'].apply(lambda x: x.split())

# Vectorize the reviews
X_w2v = np.array([vectorize_review(tokens, w2v_model) for tokens in tokenized_reviews])

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X_w2v, y, test_size=0.2, random_state=42)

# Initialize and fit the RandomForestClassifier model
rf_model = RandomForestClassifier()
rf_model.fit(X_train, y_train)

# Make predictions and evaluate
y_pred = rf_model.predict(X_test)
print("Word2Vec Accuracy:", accuracy_score(y_test, y_pred))
print("Word2Vec Classification Report:\n", classification_report(y_test, y_pred))

model = SentenceTransformer('all-mpnet-base-v2')

# Create embeddings for all reviews
X_sbert = model.encode(df['processed_review'].tolist())

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X_sbert, y, test_size=0.2, random_state=42)

# Initialize and fit the RandomForestClassifier model
rf_model = RandomForestClassifier()
rf_model.fit(X_train, y_train)

# Make predictions and evaluate
y_pred = rf_model.predict(X_test)
print("Sentence Embeddings Accuracy:", accuracy_score(y_test, y_pred))
print("Sentence Embeddings Classification Report:\n", classification_report(y_test, y_pred))

